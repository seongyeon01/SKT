{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 2021\n",
      "/tmp/ipykernel_593089/750085668.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/home/ubuntu/jupyter/SKT/log/ring_voice_binary_2/model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "from CLS_ring import CLSforRing\n",
    "from utils import load_and_resample, extract_sec, preprocess, forpred\n",
    "import torch\n",
    "import ring_voice\n",
    "device = torch.device('cuda:1')\n",
    "model = ring_voice.model\n",
    "model.load_state_dict(torch.load(\"/home/ubuntu/jupyter/SKT/log/ring_voice_binary_2/model.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 코덱별 샘플링 레이트 및 주파수 범위 설정\n",
    "codec_config = {\n",
    "    'EVS': {'sample_rate': 16000, 'freq_range': (0, 8000)},\n",
    "    'AMR-WB': {'sample_rate': 16000, 'freq_range': (0, 8000)},\n",
    "    'AMR': {'sample_rate': 8000, 'freq_range': (0, 4000)},\n",
    "}\n",
    "selected_codec = 'EVS'\n",
    "config = codec_config[selected_codec]\n",
    "target_sr = config['sample_rate']\n",
    "n_fft = 1024  # FFT 크기\n",
    "hop_length = n_fft // 4  # 일반적으로 hop_length는 n_fft의 1/4로 설정\n",
    "hann_window = torch.hann_window(n_fft)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "test_sample1_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/music.wav\"\n",
    "test_sample2_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_o.wav\" # PCM_30_wb\n",
    "test_sample3_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_x1.wav\" # PCM_15_wb\n",
    "\n",
    "test_sample4_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/music+speech.wav\"\n",
    "test_sample5_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/music+speech(noise).wav\"\n",
    "\n",
    "test_sample6_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_o+speech.wav\" # ANNC_1508_wb + start @ 60sec\n",
    "test_sample7_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_o+speech(noise).wav\" # INTL_TERM_ANM_wb + start @ 4sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2468352,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, sr = librosa.load(path, sr=16000)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = test_sample7_path\n",
    "sample = load_and_resample(path, target_sr)\n",
    "RING = CLSforRing('RF')\n",
    "s = 0\n",
    "sample_sec = extract_sec(sample, target_sr, start = int(s/target_sr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_rate = 8000\n",
    "min_duration = 1.0\n",
    "frame_length = 8064\n",
    "hop_length_frame = 8064\n",
    "n_fft = 255\n",
    "hop_length_fft = 63\n",
    "sequence_sample_length = librosa_waveform.shape[0]\n",
    "\n",
    "waveform_np = sample_sec.squeeze().numpy()\n",
    "# librosa로 처리 (sample_rate와 함께 사용 가능)\n",
    "librosa_waveform = librosa.resample(waveform_np, orig_sr=16000, target_sr=sample_rate) \n",
    "sound_data_list = [librosa_waveform[start:start + frame_length] for start in range(\n",
    "0, sequence_sample_length - frame_length + 1, hop_length_frame)]  # get sliding windows\n",
    "audio = np.vstack(sound_data_list)\n",
    "dim_square_spec = int(n_fft / 2) + 1\n",
    "m_amp_db_audio,  m_pha_audio = numpy_audio_to_matrix_spectrogram(\n",
    "        audio, dim_square_spec, n_fft, hop_length_fft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_584830/2614224937.py:6: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:305.)\n",
      "  sample_filter = preprocess(sample_sec, n_fft, hop_length, hann_window, target_sr, config['freq_range']).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1 - INTL_TERM_ANM_wb.wav\n",
      "1 : 2 - INTL_TERM_ANM_wb.wav\n",
      "2 : 3 - INTL_TERM_ANM_wb.wav\n",
      "3 : 4 - speech\n",
      "4 : 5 - speech\n",
      "5 : 6 - speech\n",
      "6 : 7 - speech\n",
      "7 : 8 - speech\n",
      "8 : 9 - speech\n",
      "9 : 10 - speech\n",
      "10 : 11 - speech\n",
      "11 : 12 - speech\n",
      "12 : 13 - speech\n",
      "13 : 14 - speech\n",
      "14 : 15 - speech\n",
      "15 : 16 - speech\n",
      "16 : 17 - speech\n",
      "17 : 18 - speech\n",
      "18 : 19 - speech\n",
      "19 : 20 - speech\n",
      "20 : 21 - speech\n",
      "21 : 22 - speech\n",
      "22 : 23 - speech\n",
      "23 : 24 - speech\n",
      "24 : 25 - speech\n",
      "25 : 26 - speech\n",
      "26 : 27 - speech\n",
      "27 : 28 - speech\n",
      "28 : 29 - speech\n",
      "29 : 30 - speech\n",
      "30 : 31 - speech\n",
      "31 : 32 - speech\n",
      "32 : 33 - speech\n",
      "33 : 34 - speech\n",
      "34 : 35 - speech\n",
      "35 : 36 - speech\n",
      "36 : 37 - speech\n",
      "37 : 38 - speech\n",
      "38 : 39 - speech\n",
      "39 : 40 - speech\n",
      "40 : 41 - speech\n",
      "41 : 42 - speech\n",
      "42 : 43 - speech\n",
      "43 : 44 - speech\n",
      "44 : 45 - speech\n",
      "45 : 46 - speech\n",
      "46 : 47 - speech\n",
      "47 : 48 - speech\n",
      "48 : 49 - speech\n",
      "49 : 50 - speech\n",
      "50 : 51 - speech\n",
      "51 : 52 - speech\n",
      "52 : 53 - speech\n",
      "53 : 54 - speech\n",
      "54 : 55 - speech\n",
      "55 : 56 - speech\n",
      "56 : 57 - speech\n",
      "57 : 58 - speech\n",
      "58 : 59 - speech\n",
      "59 : 60 - speech\n",
      "60 : 61 - speech\n",
      "61 : 62 - speech\n",
      "62 : 63 - speech\n",
      "63 : 64 - speech\n",
      "64 : 65 - speech\n",
      "65 : 66 - speech\n",
      "66 : 67 - speech\n",
      "67 : 68 - speech\n",
      "68 : 69 - speech\n",
      "69 : 70 - speech\n",
      "70 : 71 - speech\n",
      "71 : 72 - speech\n",
      "72 : 73 - speech\n",
      "73 : 74 - speech\n",
      "74 : 75 - speech\n",
      "75 : 76 - speech\n",
      "76 : 77 - speech\n",
      "77 : 78 - speech\n",
      "78 : 79 - speech\n",
      "79 : 80 - music\n",
      "80 : 81 - music\n",
      "81 : 82 - music\n",
      "82 : 83 - music\n",
      "83 : 84 - music\n",
      "84 : 85 - music\n",
      "85 : 86 - music\n",
      "86 : 87 - music\n",
      "87 : 88 - music\n",
      "88 : 89 - music\n",
      "89 : 90 - music\n",
      "90 : 91 - music\n",
      "91 : 92 - music\n",
      "92 : 93 - music\n",
      "93 : 94 - speech\n",
      "94 : 95 - music\n",
      "95 : 96 - music\n",
      "96 : 97 - music\n",
      "97 : 98 - music\n",
      "98 : 99 - speech\n",
      "99 : 100 - speech\n",
      "100 : 101 - music\n",
      "101 : 102 - speech\n",
      "102 : 103 - speech\n",
      "103 : 104 - speech\n",
      "104 : 105 - music\n",
      "105 : 106 - speech\n",
      "106 : 107 - speech\n",
      "107 : 108 - speech\n",
      "108 : 109 - speech\n",
      "109 : 110 - speech\n",
      "110 : 111 - speech\n",
      "111 : 112 - speech\n",
      "112 : 113 - speech\n",
      "113 : 114 - speech\n",
      "114 : 115 - speech\n",
      "115 : 116 - speech\n",
      "116 : 117 - speech\n",
      "117 : 118 - speech\n",
      "118 : 119 - speech\n",
      "119 : 120 - speech\n",
      "120 : 121 - speech\n",
      "121 : 122 - speech\n",
      "122 : 123 - speech\n",
      "123 : 124 - speech\n",
      "124 : 125 - speech\n",
      "125 : 126 - speech\n",
      "126 : 127 - speech\n",
      "127 : 128 - speech\n",
      "128 : 129 - speech\n",
      "129 : 130 - speech\n",
      "130 : 131 - speech\n",
      "131 : 132 - speech\n",
      "132 : 133 - speech\n",
      "133 : 134 - speech\n",
      "134 : 135 - speech\n",
      "135 : 136 - speech\n",
      "136 : 137 - speech\n",
      "137 : 138 - speech\n",
      "138 : 139 - speech\n",
      "139 : 140 - speech\n",
      "140 : 141 - speech\n",
      "141 : 142 - speech\n",
      "142 : 143 - speech\n",
      "143 : 144 - speech\n",
      "144 : 145 - speech\n",
      "145 : 146 - speech\n",
      "146 : 147 - speech\n",
      "147 : 148 - speech\n",
      "148 : 149 - speech\n",
      "149 : 150 - speech\n",
      "150 : 151 - speech\n",
      "151 : 152 - speech\n",
      "152 : 153 - speech\n",
      "153 : 154 - speech\n"
     ]
    }
   ],
   "source": [
    "from prediction_denoise import prediction\n",
    "\n",
    "path = test_sample7_path\n",
    "# wav 파일 불러오기\n",
    "sample = load_and_resample(path, target_sr)\n",
    "\n",
    "weights_path = '/home/ubuntu/jupyter/minyoung/skt/Speech-enhancement/weights/model_weights.pth'\n",
    "name_model = 'model_unet'\n",
    "audio_dir_prediction = '/home/ubuntu/jupyter/minyoung/skt/Speech-enhancement/demo_data/test'\n",
    "dir_save_prediction = '/home/ubuntu/jupyter/minyoung/skt/Speech-enhancement/demo_data/test/sample_denoise/'\n",
    "audio_input_prediction = [path]\n",
    "audio_output_prediction = 'sample32_denoise.wav'\n",
    "sample_rate = 8000\n",
    "min_duration = 1.0\n",
    "frame_length = 8064\n",
    "hop_length_frame = 8064\n",
    "n_fft = 255\n",
    "hop_length_fft = 63\n",
    "\n",
    "# Prediction 및 enhancement 단계\n",
    "prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction,\n",
    "            audio_input_prediction, audio_output_prediction, sample_rate, min_duration,\n",
    "            frame_length, hop_length_frame, n_fft, hop_length_fft)\n",
    "\n",
    "# 링백톤 분류기 불러오기\n",
    "RING = CLSforRing('RF')\n",
    "\n",
    "# 1초 단위로 예측\n",
    "for s in range(0, sample.shape[-1]- target_sr, target_sr):\n",
    "    sample_sec = extract_sec(sample, target_sr, start = int(s/target_sr))\n",
    "    sample_filter = preprocess(sample_sec, n_fft, hop_length, hann_window, target_sr, config['freq_range']).float()\n",
    "    is_sys = RING.cls(sample_filter.reshape(1,-1))\n",
    "    if is_sys == 'no sys':\n",
    "        with torch.no_grad():\n",
    "            out_dict = model(forpred(sample_filter.unsqueeze(0)).unsqueeze(0).type(torch.FloatTensor).transpose(2,3).to(device))\n",
    "        test_pred=out_dict['event_logit']\n",
    "        no_speech = torch.argmax(test_pred, dim = -1).item()\n",
    "        if no_speech:\n",
    "            print(f\"{int(s/target_sr)} : {int(s/target_sr)+1} - music\") \n",
    "        else:\n",
    "            #######################\n",
    "            ### Enhancement\n",
    "            #######################\n",
    "            # 경로 및 파라미터들 직접 작성\n",
    "            \n",
    "\n",
    "            print(f\"{int(s/target_sr)} : {int(s/target_sr)+1} - speech\")\n",
    "\n",
    "    else:\n",
    "        print(f\"{int(s/target_sr)} : {int(s/target_sr)+1} - {is_sys}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skt_py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
