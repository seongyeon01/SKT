{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 13:44:48.167751: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-22 13:44:48.221517: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-22 13:44:49.333669: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/tmp/ipykernel_669395/143490764.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/home/ubuntu/jupyter/SKT/log/ring_voice_binary_2/model.pt\"))\n"
     ]
    }
   ],
   "source": [
    "from CLS_ring import CLSforRing\n",
    "from utils import load_and_resample, extract_sec, preprocess, forpred\n",
    "import torch\n",
    "import ring_voice\n",
    "from prediction_denoise import prediction\n",
    "device = torch.device('cuda:1')\n",
    "model = ring_voice.model\n",
    "model.load_state_dict(torch.load(\"/home/ubuntu/jupyter/SKT/log/ring_voice_binary_2/model.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 코덱별 샘플링 레이트 및 주파수 범위 설정\n",
    "codec_config = {\n",
    "    'EVS': {'sample_rate': 16000, 'freq_range': (0, 8000)},\n",
    "    'AMR-WB': {'sample_rate': 16000, 'freq_range': (0, 8000)},\n",
    "    'AMR': {'sample_rate': 8000, 'freq_range': (0, 4000)},\n",
    "}\n",
    "selected_codec = 'EVS'\n",
    "config = codec_config[selected_codec]\n",
    "target_sr = config['sample_rate']\n",
    "n_fft = 1024  # FFT 크기\n",
    "hop_length = n_fft // 4  # 일반적으로 hop_length는 n_fft의 1/4로 설정\n",
    "hann_window = torch.hann_window(n_fft)\n",
    "\n",
    "weights_path = '/home/ubuntu/jupyter/SKT/submit/weights'\n",
    "name_model = 'model_unet'\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "test_sample1_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/music.wav\"\n",
    "test_sample2_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_o.wav\" # PCM_30_wb\n",
    "test_sample3_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_x1.wav\" # PCM_15_wb\n",
    "\n",
    "test_sample4_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/music+speech.wav\"\n",
    "test_sample5_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/music+speech(noise).wav\"\n",
    "\n",
    "test_sample6_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_o+speech.wav\" # ANNC_1508_wb + start @ 60sec\n",
    "test_sample7_path = \"/home/ubuntu/jupyter/SKT/data/Scenario/system_o+speech(noise).wav\" # INTL_TERM_ANM_wb + start @ 4sec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " /home/ubuntu/jupyter/SKT/data/Scenario/music.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_669395/323420719.py:7: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at ../aten/src/ATen/native/Copy.cpp:305.)\n",
      "  sample_filter = preprocess(sample_sec, n_fft, hop_length, hann_window, target_sr, config['freq_range']).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1 - music\n",
      "1 : 2 - music\n",
      "2 : 3 - music\n",
      "3 : 4 - music\n",
      "4 : 5 - music\n",
      "5 : 6 - speech\n",
      "\n",
      " /home/ubuntu/jupyter/SKT/data/Scenario/system_o.wav\n",
      "0 : 1 - PCM_6_wb.wav\n",
      "1 : 2 - PCM_30_wb.wav\n",
      "2 : 3 - PCM_30_wb.wav\n",
      "3 : 4 - PCM_30_wb.wav\n",
      "\n",
      " /home/ubuntu/jupyter/SKT/data/Scenario/system_x1.wav\n",
      "0 : 1 - PCM_6_wb.wav\n",
      "1 : 2 - PCM_15_wb.wav\n",
      "2 : 3 - PCM_15_wb.wav\n",
      "3 : 4 - PCM_15_wb.wav\n",
      "4 : 5 - PCM_15_wb.wav\n",
      "5 : 6 - PCM_15_wb.wav\n",
      "6 : 7 - PCM_15_wb.wav\n",
      "7 : 8 - PCM_15_wb.wav\n",
      "8 : 9 - PCM_15_wb.wav\n",
      "9 : 10 - PCM_15_wb.wav\n",
      "10 : 11 - PCM_15_wb.wav\n",
      "11 : 12 - PCM_15_wb.wav\n",
      "\n",
      " /home/ubuntu/jupyter/SKT/data/Scenario/music+speech.wav\n",
      "0 : 1 - music\n",
      "1 : 2 - music\n",
      "2 : 3 - music\n",
      "3 : 4 - music\n",
      "4 : 5 - music\n",
      "5 : 6 - music\n",
      "6 : 7 - music\n",
      "7 : 8 - music\n",
      "8 : 9 - music\n",
      "9 : 10 - music\n",
      "10 : 11 - music\n",
      "11 : 12 - music\n",
      "12 : 13 - music\n",
      "13 : 14 - music\n",
      "14 : 15 - music\n",
      "15 : 16 - music\n",
      "16 : 17 - music\n",
      "17 : 18 - music\n",
      "18 : 19 - music\n",
      "19 : 20 - music\n",
      "20 : 21 - music\n",
      "21 : 22 - music\n",
      "22 : 23 - music\n",
      "23 : 24 - music\n",
      "24 : 25 - music\n",
      "25 : 26 - music\n",
      "26 : 27 - music\n",
      "27 : 28 - music\n",
      "28 : 29 - music\n",
      "29 : 30 - music\n",
      "30 : 31 - speech\n",
      "\n",
      " /home/ubuntu/jupyter/SKT/data/Scenario/music+speech(noise).wav\n",
      "0 : 1 - music\n",
      "1 : 2 - speech\n",
      "\n",
      " /home/ubuntu/jupyter/SKT/data/Scenario/system_o+speech.wav\n",
      "0 : 1 - ANNC_1508_wb.wav\n",
      "1 : 2 - ANNC_1508_wb.wav\n",
      "2 : 3 - ANNC_1508_wb.wav\n",
      "3 : 4 - ANNC_1508_wb.wav\n",
      "4 : 5 - ANNC_1508_wb.wav\n",
      "5 : 6 - ANNC_1508_wb.wav\n",
      "6 : 7 - ANNC_1508_wb.wav\n",
      "7 : 8 - ANNC_1508_wb.wav\n",
      "8 : 9 - ANNC_1508_wb.wav\n",
      "9 : 10 - ANNC_1508_wb.wav\n",
      "10 : 11 - ANNC_1508_wb.wav\n",
      "11 : 12 - ANNC_1508_wb.wav\n",
      "12 : 13 - ANNC_1508_wb.wav\n",
      "13 : 14 - ANNC_1508_wb.wav\n",
      "14 : 15 - ANNC_1508_wb.wav\n",
      "15 : 16 - ANNC_1508_wb.wav\n",
      "16 : 17 - ANNC_1508_wb.wav\n",
      "17 : 18 - ANNC_1508_wb.wav\n",
      "18 : 19 - ANNC_1508_wb.wav\n",
      "19 : 20 - ANNC_1508_wb.wav\n",
      "20 : 21 - ANNC_1508_wb.wav\n",
      "21 : 22 - ANNC_1508_wb.wav\n",
      "22 : 23 - ANNC_1508_wb.wav\n",
      "23 : 24 - ANNC_1508_wb.wav\n",
      "24 : 25 - ANNC_1508_wb.wav\n",
      "25 : 26 - ANNC_1508_wb.wav\n",
      "26 : 27 - ANNC_1508_wb.wav\n",
      "27 : 28 - ANNC_1508_wb.wav\n",
      "28 : 29 - ANNC_1508_wb.wav\n",
      "29 : 30 - ANNC_1508_wb.wav\n",
      "30 : 31 - ANNC_1508_wb.wav\n",
      "31 : 32 - ANNC_1508_wb.wav\n",
      "32 : 33 - ANNC_1508_wb.wav\n",
      "33 : 34 - ANNC_1508_wb.wav\n",
      "34 : 35 - ANNC_1508_wb.wav\n",
      "35 : 36 - ANNC_1508_wb.wav\n",
      "36 : 37 - ANNC_1508_wb.wav\n",
      "37 : 38 - ANNC_1508_wb.wav\n",
      "38 : 39 - ANNC_1508_wb.wav\n",
      "39 : 40 - ANNC_1508_wb.wav\n",
      "40 : 41 - ANNC_1508_wb.wav\n",
      "41 : 42 - ANNC_1508_wb.wav\n",
      "42 : 43 - ANNC_1508_wb.wav\n",
      "43 : 44 - ANNC_1508_wb.wav\n",
      "44 : 45 - ANNC_1508_wb.wav\n",
      "45 : 46 - ANNC_1508_wb.wav\n",
      "46 : 47 - ANNC_1508_wb.wav\n",
      "47 : 48 - ANNC_1508_wb.wav\n",
      "48 : 49 - ANNC_1508_wb.wav\n",
      "49 : 50 - ANNC_1508_wb.wav\n",
      "50 : 51 - ANNC_1508_wb.wav\n",
      "51 : 52 - ANNC_1508_wb.wav\n",
      "52 : 53 - ANNC_1508_wb.wav\n",
      "53 : 54 - ANNC_1508_wb.wav\n",
      "54 : 55 - ANNC_1508_wb.wav\n",
      "55 : 56 - ANNC_1508_wb.wav\n",
      "56 : 57 - ANNC_1508_wb.wav\n",
      "57 : 58 - ANNC_1508_wb.wav\n",
      "58 : 59 - ANNC_1508_wb.wav\n",
      "59 : 60 - music\n",
      "60 : 61 - speech\n",
      "\n",
      " /home/ubuntu/jupyter/SKT/data/Scenario/system_o+speech(noise).wav\n",
      "0 : 1 - INTL_TERM_ANM_wb.wav\n",
      "1 : 2 - INTL_TERM_ANM_wb.wav\n",
      "2 : 3 - INTL_TERM_ANM_wb.wav\n",
      "3 : 4 - speech\n"
     ]
    }
   ],
   "source": [
    "for path in [test_sample1_path, test_sample2_path, test_sample3_path, test_sample4_path, test_sample5_path, test_sample6_path, test_sample7_path]:\n",
    "    print(\"\\n\",path)\n",
    "    sample = load_and_resample(path, target_sr)\n",
    "    RING = CLSforRing('RF')\n",
    "    for s in range(0, sample.shape[-1]- target_sr, target_sr):\n",
    "        sample_sec = extract_sec(sample, target_sr, start = int(s/target_sr))\n",
    "        sample_filter = preprocess(sample_sec, n_fft, hop_length, hann_window, target_sr, config['freq_range']).float()\n",
    "        is_sys = RING.cls(sample_filter.reshape(1,-1))\n",
    "        if is_sys == 'no sys':\n",
    "            with torch.no_grad():\n",
    "                out_dict = model(forpred(sample_filter.unsqueeze(0)).unsqueeze(0).type(torch.FloatTensor).transpose(2,3).to(device))\n",
    "            test_pred=out_dict['event_logit']\n",
    "            no_speech = torch.argmax(test_pred, dim = -1).item()\n",
    "            if no_speech:\n",
    "                print(f\"{int(s/target_sr)} : {int(s/target_sr)+1} - music\") \n",
    "            else:\n",
    "                print(f\"{int(s/target_sr)} : {int(s/target_sr)+1} - speech\")\n",
    "                break\n",
    "                # truncated_sample = sample[:, s:]\n",
    "                # #######################\n",
    "                # ### Enhancement\n",
    "                # #######################\n",
    "                # # 경로 및 파라미터들 직접 작성\n",
    "                \n",
    "                # audio_dir_prediction = '/home/ubuntu/jupyter/minyoung/skt/Speech-enhancement/demo_data/test'\n",
    "                # dir_save_prediction = '/home/ubuntu/jupyter/minyoung/skt/Speech-enhancement/demo_data/test/sample_denoise/'\n",
    "                # audio_input_prediction = [truncated_sample]\n",
    "                # audio_output_prediction = 'sample32_denoise.wav'\n",
    "                # # Prediction 및 enhancement 단계\n",
    "                # prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction,\n",
    "                #             audio_input_prediction, audio_output_prediction, 8000, 1.0,\n",
    "                #             8064, 8064, 255, 63)\n",
    "\n",
    "        else:\n",
    "            print(f\"{int(s/target_sr)} : {int(s/target_sr)+1} - {is_sys}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'adgadg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43madgadg\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'adgadg' is not defined"
     ]
    }
   ],
   "source": [
    "adgadg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# import os\n",
    "\n",
    "# def split_audio(file_path, output_folder, split_length_ms=1000):\n",
    "#     # 오디오 파일 로드\n",
    "#     audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "#     # 전체 길이 계산 (밀리초 단위)\n",
    "#     total_length_ms = len(audio)\n",
    "\n",
    "#     # 출력을 저장할 폴더가 없다면 생성\n",
    "#     if not os.path.exists(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "\n",
    "#     # 1초 단위로 오디오 자르기\n",
    "#     for i in range(0, total_length_ms, split_length_ms):\n",
    "#         split_audio = audio[i:i+split_length_ms]\n",
    "#         # 파일 저장 (0001, 0002 형식으로 저장)\n",
    "#         split_filename = os.path.join(output_folder, f\"split_{i//split_length_ms:04d}.wav\")\n",
    "#         split_audio.export(split_filename, format=\"wav\")\n",
    "#         print(f\"저장됨: {split_filename}\")\n",
    "\n",
    "# # 예시 사용법\n",
    "# for file_path in [test_sample2_path, test_sample3_path, test_sample4_path, test_sample5_path, test_sample6_path, test_sample7_path]:\n",
    "#     os.makedirs(f\"/home/ubuntu/jupyter/SKT/submit/samples/{os.path.basename(file_path)[:-4]}\", exist_ok = True)\n",
    "#     output_folder = f\"/home/ubuntu/jupyter/SKT/submit/samples/{os.path.basename(file_path)[:-4]}\"\n",
    "#     split_audio(file_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pydub import AudioSegment\n",
    "# import os\n",
    "# ring_path = \"/home/ubuntu/jupyter/SKT/data/ring\"\n",
    "# ring_list = os.listdir(ring_path)\n",
    "\n",
    "# def split_audio(file_path, output_folder, split_length_ms=1000):\n",
    "#     # 오디오 파일 로드\n",
    "#     audio = AudioSegment.from_file(file_path)\n",
    "\n",
    "#     # 전체 길이 계산 (밀리초 단위)\n",
    "#     total_length_ms = len(audio)\n",
    "\n",
    "#     # 출력을 저장할 폴더가 없다면 생성\n",
    "#     if not os.path.exists(output_folder):\n",
    "#         os.makedirs(output_folder)\n",
    "\n",
    "#     # 1초 단위로 오디오 자르기\n",
    "#     for i in range(0, total_length_ms, split_length_ms):\n",
    "#         split_audio = audio[i:i+split_length_ms]\n",
    "#         # 파일 저장 (0001, 0002 형식으로 저장)\n",
    "#         split_filename = os.path.join(output_folder, f\"{os.path.basename(file_path)[:-4]}{i//split_length_ms:04d}.wav\")\n",
    "#         split_audio.export(split_filename, format=\"wav\")\n",
    "#         print(f\"저장됨: {split_filename}\")\n",
    "\n",
    "# # 예시 사용법\n",
    "# for file_path in ring_list:\n",
    "#     output_folder = f\"/home/ubuntu/jupyter/SKT/data/ring_split\"\n",
    "#     os.makedirs(output_folder, exist_ok = True)\n",
    "#     split_audio(f\"{ring_path}/{file_path}\", output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pickle\n",
    "# from tqdm.notebook import tqdm\n",
    "# ring_path = f\"/home/ubuntu/jupyter/SKT/data/ring_split\" # path for system ringbacktone\n",
    "# ring_list = [f\"{ring_path}/{w}\" for w in os.listdir(ring_path)] # 1초 이하 제거\n",
    "# # ring2idx = {r:i for i, r in enumerate(ring_list)}\n",
    "# # idx2ring = {i:os.path.basename(r) for i, r in enumerate(ring_list)}\n",
    "\n",
    "# ring_specs = []\n",
    "# idx2ring = {}\n",
    "# i = 0\n",
    "# for r in tqdm(ring_list):\n",
    "#     ring_sec = load_and_resample(r, target_sr)\n",
    "#     print(ring_sec.shape)\n",
    "#     try:\n",
    "#         ring_filter = preprocess(ring_sec, n_fft, hop_length, hann_window, target_sr, config['freq_range']).float()\n",
    "#     except:\n",
    "#         continue\n",
    "#     spec = torch.zeros([513, 63])\n",
    "#     spec[:,:ring_filter.shape[-1]] = ring_filter.squeeze()\n",
    "#     ring_specs.append(spec)\n",
    "#     idx2ring[i] = '_'.join(os.path.basename(r)[:-4].split('_')[:-1])\n",
    "#     i+=1\n",
    "\n",
    "# # np.save('ring_label2.npy', np.array(labels))\n",
    "# np.save('ring_spec2.npy', torch.stack(ring_specs).numpy())\n",
    "# with open('idx2ring2.pickle','wb') as fw:\n",
    "#     pickle.dump(idx2ring, fw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from prediction_denoise import prediction\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import librosa\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "# weights_path = '/home/ubuntu/jupyter/SKT/submit/weights'\n",
    "# name_model = 'model_unet'\n",
    "\n",
    "\n",
    "# sample_dir = \"/home/ubuntu/jupyter/SKT/submit/samples\"\n",
    "# test_list = os.listdir(sample_dir)\n",
    "\n",
    "# for test_folder in test_list:\n",
    "#     print('\\n',test_folder)\n",
    "\n",
    "#     # 링백톤 분류기 불러오기\n",
    "#     RING = CLSforRing()\n",
    "#     # 1초 단위로 wav 파일 불러오기\n",
    "#     ## 1초 단위 wav list\n",
    "#     test_samples = sorted(os.listdir(f\"{sample_dir}/{test_folder}\"))\n",
    "#     s = 0\n",
    "#     for path in test_samples:\n",
    "#         sample_sec = load_and_resample(f\"{sample_dir}/{test_folder}/{path}\", target_sr)\n",
    "#         sample_filter = torch.empty([513, 63])\n",
    "#         sample_filter_ = preprocess(sample_sec, n_fft, hop_length, hann_window, target_sr, config['freq_range']).float()\n",
    "#         sample_filter[:,:sample_filter_.shape[-1]] = sample_filter_\n",
    "#         is_sys = RING.cls(sample_filter.reshape(1,-1))\n",
    "#         if is_sys == 'no sys':\n",
    "#             with torch.no_grad():\n",
    "#                 out_dict = model(forpred(sample_filter.unsqueeze(0)).unsqueeze(0).type(torch.FloatTensor).transpose(2,3).to(device))\n",
    "#             test_pred=out_dict['event_logit']\n",
    "#             no_speech = torch.argmax(test_pred, dim = -1).item()\n",
    "#             if no_speech:\n",
    "#                 print(f\"{s} : {s+1} - music\") \n",
    "#             else:\n",
    "#                 print(f\"{s} : {s+1} - speech\")\n",
    "#                 #######################\n",
    "#                 ### Enhancement\n",
    "#                 #######################\n",
    "#                 # 경로 및 파라미터들 직접 작성\n",
    "#                 audio_dir_prediction = f\"{sample_dir}/{test_folder}/\"\n",
    "#                 dir_save_prediction = audio_dir_prediction.replace('samples','enhance')\n",
    "#                 audio_input_prediction = [path]\n",
    "#                 audio_output_prediction = path\n",
    "#                 # Prediction 및 enhancement 단계\n",
    "#                 prediction(weights_path, name_model, audio_dir_prediction, dir_save_prediction,\n",
    "#                             audio_input_prediction, audio_output_prediction, 8000, 1.0,\n",
    "#                             8064, 8064, 255, 63)\n",
    "#         else:\n",
    "#             print(f\"{s} : {s+1} - {is_sys}\") \n",
    "#         s += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skt_yeon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
